{
	"name": "Notebook 1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "slimanepool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4ce01fce-f9ad-4548-82a0-58db2ae8aab0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/59b3f2eb-caba-469e-afca-cde678d49c24/resourceGroups/skarzal/providers/Microsoft.Synapse/workspaces/slimanesynpse/bigDataPools/slimanepool",
				"name": "slimanepool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				}
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"%%pyspark\r\n",
					"df = spark.read.load('abfss://raw@slimanedatalake.dfs.core.windows.net/yellow_tripdata_2022-03.parquet', format='parquet')\r\n",
					"\r\n",
					"display(df.limit(10))"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import col, sum\r\n",
					"\r\n",
					"display(df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]))\r\n",
					""
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(df.filter(col('passenger_count').isNull()))"
				],
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(df.filter(col('passenger_count').isNotNull()))\r\n",
					""
				],
				"execution_count": 60
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(df.groupBy(\"passenger_count\").count().orderBy(col(\"count\").desc()))"
				],
				"execution_count": 61
			},
			{
				"cell_type": "code",
				"source": [
					"df = df.filter(col(\"passenger_count\") <= 6)"
				],
				"execution_count": 62
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(df.groupBy(\"passenger_count\").count().orderBy(col(\"count\").desc()))"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"source": [
					"df = df.filter(col(\"payment_type\") != 0)"
				],
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"source": [
					"df  = df.filter(col(\"RateCodeID\").between(1, 6))"
				],
				"execution_count": 65
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(df.limit(10))"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import col, sum\r\n",
					"display(df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]))"
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"# Créer un identifiant unique pour chaque jour (YYYYMMDD)\r\n",
					"dim_dates = df.select(\r\n",
					"    F.date_format('tpep_pickup_datetime', 'yyyyMMdd').alias('date_id'),  # Identifiant unique de la date (YYYYMMDD)\r\n",
					"    F.to_date('tpep_pickup_datetime').alias('date'),  # Date complète (yyyy-MM-dd)\r\n",
					"    F.dayofweek('tpep_pickup_datetime').alias('day_of_week'),  # Jour de la semaine (1 = dimanche, 7 = samedi)\r\n",
					"    F.month('tpep_pickup_datetime').alias('month'),  # Mois\r\n",
					"    F.quarter('tpep_pickup_datetime').alias('quarter'),  # Trimestre\r\n",
					"    F.year('tpep_pickup_datetime').alias('year')  # Année\r\n",
					")"
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"dim_dates.show(truncate=False)"
				],
				"execution_count": 69
			},
			{
				"cell_type": "code",
				"source": [
					"dim_vendors = df.select(\r\n",
					"    'VendorID',\r\n",
					"    F.when(df['VendorID'] == 1, 'Creative Mobile Technologies, LLC')\r\n",
					"     .when(df['VendorID'] == 2, 'VeriFone Inc.')\r\n",
					"     .alias('vendor_name') \r\n",
					").dropDuplicates()  \r\n",
					"\r\n",
					"dim_vendors.show(truncate=False)"
				],
				"execution_count": 70
			},
			{
				"cell_type": "code",
				"source": [
					"dim_payment_types = df.select(\r\n",
					"    'payment_type',\r\n",
					"    F.when(df['payment_type'] == 1, 'Credit card')\r\n",
					"     .when(df['payment_type'] == 2, 'Cash')\r\n",
					"     .when(df['payment_type'] == 3, 'No charge')\r\n",
					"     .when(df['payment_type'] == 4, 'Dispute')\r\n",
					"     .when(df['payment_type'] == 5, 'Unknown')\r\n",
					"     .when(df['payment_type'] == 6, 'Voided trip')\r\n",
					"     .alias('payment_type_name')\r\n",
					").dropDuplicates() \r\n",
					"\r\n",
					"dim_payment_types.show(truncate=False)"
				],
				"execution_count": 71
			},
			{
				"cell_type": "code",
				"source": [
					"dim_rate_codes = df.select(\r\n",
					"    'RateCodeID',\r\n",
					"    F.when(df['RateCodeID'] == 1, 'Standard rate')\r\n",
					"     .when(df['RateCodeID'] == 2, 'JFK')\r\n",
					"     .when(df['RateCodeID'] == 3, 'Newark')\r\n",
					"     .when(df['RateCodeID'] == 4, 'Nassau or Westchester')\r\n",
					"     .when(df['RateCodeID'] == 5, 'Negotiated fare')\r\n",
					"     .when(df['RateCodeID'] == 6, 'Group ride')\r\n",
					"     .alias('rate_code_name')\r\n",
					").dropDuplicates()  \r\n",
					"dim_rate_codes.show(truncate=False)\r\n",
					""
				],
				"execution_count": 72
			},
			{
				"cell_type": "code",
				"source": [
					"zones = spark.read.option(\"header\", \"true\").csv('abfss://raw@slimanedatalake.dfs.core.windows.net/taxi_zone_lookup.csv')\r\n",
					"\r\n",
					"zones.show()\r\n",
					""
				],
				"execution_count": 77
			},
			{
				"cell_type": "code",
				"source": [
					"dim_location = zones.select(\r\n",
					"    'LocationID', \r\n",
					"    'Borough', \r\n",
					"    'Zone', \r\n",
					"    'service_zone'\r\n",
					").distinct()"
				],
				"execution_count": 78
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(dim_location.limit(10))"
				],
				"execution_count": 82
			},
			{
				"cell_type": "code",
				"source": [
					"taxi_data = df"
				],
				"execution_count": 85
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"fact_table = df.select(\r\n",
					"    col(\"VendorID\"),\r\n",
					"    col(\"tpep_pickup_datetime\"),\r\n",
					"    col(\"tpep_dropoff_datetime\"),\r\n",
					"    col(\"passenger_count\"),\r\n",
					"    col(\"trip_distance\"),\r\n",
					"    col(\"RatecodeID\"),\r\n",
					"    col(\"store_and_fwd_flag\"),\r\n",
					"    col(\"PULocationID\"),\r\n",
					"    col(\"DOLocationID\"),\r\n",
					"    col(\"payment_type\"),\r\n",
					"    col(\"fare_amount\"),\r\n",
					"    col(\"extra\"),\r\n",
					"    col(\"mta_tax\"),\r\n",
					"    col(\"tip_amount\"),\r\n",
					"    col(\"tolls_amount\"),\r\n",
					"    col(\"improvement_surcharge\"),\r\n",
					"    col(\"total_amount\"),\r\n",
					"    col(\"congestion_surcharge\"),\r\n",
					"    col(\"airport_fee\")\r\n",
					")\r\n",
					"\r\n",
					"# Vérifier les résultats\r\n",
					"display(fact_table)"
				],
				"execution_count": 88
			},
			{
				"cell_type": "code",
				"source": [
					"data_lake_path = \"abfss://curated@slimanedatalake.dfs.core.windows.net/\""
				],
				"execution_count": 92
			},
			{
				"cell_type": "code",
				"source": [
					"dim_vendors.write.mode(\"overwrite\").parquet(f\"{data_lake_path}dim_vendors.parquet\")\r\n",
					"dim_dates.write.mode(\"overwrite\").parquet(f\"{data_lake_path}dim_dates.parquet\")\r\n",
					"dim_location.write.mode(\"overwrite\").parquet(f\"{data_lake_path}dim_locations.parquet\")"
				],
				"execution_count": 93
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}